import requests
import pandas as pd
import os
import time

def crawl_104_jobs(pages=10):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36',
        'Referer': 'https://www.104.com.tw/jobs/search/',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'X-Requested-With': 'XMLHttpRequest'
    }

    url = "https://www.104.com.tw/jobs/search/list"
    all_jobs = []

    for page in range(1, pages + 1):
        params = {
            "ro": 0,  # å…¨è·ã€å…¼è·ã€å¯¦ç¿’éƒ½è¦
            "keyword": "å¯¦ç¿’",
            "mode": "l",
            "order": 11,  # æœ€æ–°æ’åº
            "asc": 0,     # é™åº
            "page": page
        }

        try:
            resp = requests.get(url, headers=headers, params=params)
            if resp.status_code == 200:
                data = resp.json()
                jobs = data.get('data', {}).get('list', [])
                if not jobs:
                    print(f"ç¬¬ {page} é ç„¡è·ç¼ºï¼ŒçµæŸçˆ¬å–")
                    break
                all_jobs.extend(jobs)
                print(f"âœ… æˆåŠŸçˆ¬å–ç¬¬ {page} é ï¼Œå…±ç²å¾— {len(jobs)} ç­†è·ç¼º")
            else:
                print(f"âŒ ç¬¬ {page} é è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{resp.status_code}")
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {page} é ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

        time.sleep(2)

    return all_jobs

def save_jobs_to_csv(jobs, filename='pages/104_jobs.csv'):
    if not os.path.exists('pages'):
        os.makedirs('pages')

    df = pd.DataFrame(jobs)
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ å·²å„²å­˜ {len(jobs)} ç­†è·ç¼ºè³‡æ–™åˆ° {filename}")

if __name__ == "__main__":
    job_list = crawl_104_jobs(10)
    save_jobs_to_csv(job_list)
