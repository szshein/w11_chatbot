# pages/job_cleaner.py
# -*- coding: utf-8 -*-
"""
ğŸ§¹Job Cleaner - è®€å…¥è·ç¼ºCSVï¼Œå¾è·ç¼ºæè¿°æ“·å–å‡ºã€Œå·¥ä½œå…§å®¹ã€ï¼ŒåŒ¯å‡º
"""
import os
import re
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from autogen import AssistantAgent, LLMConfig
from tqdm import tqdm

# ----------------------------- åŸºæœ¬è¨­å®š --------------------------------------
PAGE_TITLE = "ğŸ§¹Job Cleaner"
st.set_page_config(page_title=PAGE_TITLE, layout="wide", page_icon="ğŸ§¹")
st.title(PAGE_TITLE)

def paging():
    st.page_link("streamlit_app.py", label="Home", icon="ğŸ ")
    st.page_link("pages/two_agents.py", label="Two Agents' Talk", icon="ğŸ’­")
    st.page_link("pages/job_cleaner.py", label="Job Cleaner", icon="ğŸ§¹")

with st.sidebar:
    paging()

load_dotenv(override=True)
API_KEY = os.getenv("GEMINI_API_KEY")

# ----------------------------- å»ºç«‹ LLM Agent --------------------------------
llm_cfg = LLMConfig(
    api_type="google",
    model="gemini-2.0-pro",
    api_key=API_KEY,
    temperature=0,
    max_tokens=256
)

extractor = AssistantAgent(
    name="extractor",
    llm_config=llm_cfg,
    system_message=(
        "ä½ æ˜¯ä¸€ä½äººåŠ›è³‡æºè³‡æ–™æ¸…ç†åŠ©æ‰‹ã€‚\n"
        "è¼¸å…¥æ˜¯ä¸€æ®µè·ç¼ºç°¡ä»‹ï¼Œè«‹ï¼š\n"
        "1. æ‰¾å‡ºã€å·¥ä½œå…§å®¹/è¦‹ç¿’å…§å®¹/Responsibilitiesã€æ®µè½ä¸¦ä¿ç•™æ¢åˆ—ã€‚\n"
        "2. ç§»é™¤è³‡æ ¼æ¢ä»¶ã€æœŸé–“èªªæ˜ã€è­‰æ˜ã€æ³¨æ„äº‹é …ç­‰ã€‚\n"
        "3. ç›´æ¥è¼¸å‡ºç´”æ–‡å­—ï¼Œä¸è¦é™„åŠ å…¶ä»–èªªæ˜ã€‚"
    ),
    max_consecutive_auto_reply=1
)

@st.cache_data(show_spinner=False)
def extract_content(desc: str) -> str:
    """
    å‘¼å« LLM æŠ½å–å·¥ä½œå…§å®¹ï¼›å¦‚æœå›ç©ºå­—ä¸²å°±ç”¨ regex å‚™æ´ã€‚
    """
    if not isinstance(desc, str) or desc.strip() == "":
        return ""

    # â¬‡ï¸ ç”¨ generate_reply å–å¾—å›ç­”
    reply_msg = extractor.generate_reply(
        messages=[{"role": "user", "content": desc}],
        sender="user"                    # èªªæ˜é€™æ˜¯ user ç™¼è©±
    )
    reply = reply_msg["content"].strip() if reply_msg else ""

    if reply:
        return reply

    # ---------- fallback: regex ------------------------------------------------
    m = re.search(r"([(ï¼ˆ]?\s*1[.)ï¼‰]\s*.*?)(?:\n\s*\n|$)", desc, flags=re.S)
    return m.group(1).strip() if m else ""

# ----------------------------- UI ä»‹é¢ ---------------------------------------
uploaded = st.file_uploader("â¬†ï¸ ä¸Šå‚³å« jobName, description æ¬„ä½çš„ CSV", type="csv")

if uploaded:
    df = pd.read_csv(uploaded)
    if {"jobName", "description"}.issubset(df.columns):
        st.success(f"è¼‰å…¥ {len(df)} ç­†è·ç¼ºï¼Œé–‹å§‹æ¸…ç†â€¦")
        progress = st.progress(0, text="LLM æŠ½å–ä¸­â€¦")
        contents = [""] * len(df)

        # ä¸¦è¡Œå‘¼å«ï¼Œæé«˜é€Ÿåº¦
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {
                executor.submit(extract_content, desc): idx
                for idx, desc in enumerate(df["description"])
            }
            for n, future in enumerate(as_completed(futures), start=1):
                idx = futures[future]
                try:
                    contents[idx] = future.result()
                except Exception as e:
                    contents[idx] = ""
                    st.warning(f"ç¬¬ {idx} ç­†å¤±æ•—ï¼š{e}")
                progress.progress(n / len(futures))

        df["jobContent"] = contents
        st.success("âœ… æŠ½å–å®Œæˆï¼")

        # é¡¯ç¤ºå‰ 5 ç­†é è¦½
        st.subheader("é è¦½ (å‰ 5 ç­†)")
        st.dataframe(df[["jobName", "jobContent"]].head(), use_container_width=True)

        # ä¸‹è¼‰æŒ‰éˆ•
        cleaned_csv = df[["jobName", "jobContent"]].to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ğŸ’¾ ä¸‹è¼‰æ¸…ç†å¾Œ CSV",
            cleaned_csv,
            file_name="jobs_cleaned.csv",
            mime="text/csv"
        )
    else:
        st.error("CSV å¿…é ˆåŒ…å« 'jobName' èˆ‡ 'description' æ¬„ä½ï¼")
else:
    st.info("è«‹å…ˆä¸Šå‚³ CSV æª”")